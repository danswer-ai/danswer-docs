---
title: HuggingFace Inference API
description: "Configure Onyx to use HuggingFace APIs"
---

Refer to [Model Configs](https://docs.onyx.app/gen_ai_configs/overview#model-configs) for how to set the
environment variables for your particular deployment.

To use the HuggingFace Inference APIs, you must sign up for a `Pro Account` to get an API Key

1. After signing up for `Pro Account`, go to your user settings:

![HFSettings](/images/gen_ai/HFSettings.png)

2. Copy the `User Access Token`

![HFAccessToken](/images/gen_ai/HFAccessToken.png)

## Set Onyx to use `Llama-2-70B` via next-token generation prompting

On the `LLM` page in the Admin Panel add a `Custom LLM Provider` with the following settings:

![HFCustomLLMProvider1](/images/gen_ai/HFCustomLLMProvider1.png)

![HFCustomLLMProvider2](/images/gen_ai/HFCustomLLMProvider2.png)

**Update (November 2023)**: HuggingFace has stopped supporting very large models (>10GB) via the _Pro Plan_.
The latest options are to rent dedicated hardware from them via Inference Endpoint or get an Enterprise Plan.
The _Pro Plan_ still supports smaller models but these produce worse results for Onyx.
