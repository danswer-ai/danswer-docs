---
title: "EKS"
description: "Setup Onyx on AWS EKS"
---

## Prerequisites

- Download and install the [AWS CLI](https://aws.amazon.com/cli/). This is required for creating and accessing the cluster.
- Download and install [kubectl CLI](https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/). This is needed for cluster access through the cli.

## Cluster Setup and Configuration

### Create the Cluster

Navigate to Elastic Kubernetes Service (EKS) and create a new cluster.

For `Cluster service role`, create a new IAM role accepting all the defaults for the user. It's name can
be something like `onyx-eks-cluster-role`.
Make sure to click the refresh button if you don't see it as an option!

For the `Kubernetes version`, select a version with standard support still offered and under `Upgrade policy`, select `Standard`. In the add-ons section, make sure to add the `Amazon EBS CSI Driver` add-on, as this is needed for the Persistent Volume Claims (PVCs) to be fulfilled. Also make sure to keep the other default add-ons!

![Cluster creation screen](/images/setup_guides/aws/eks/cluster_create.png)

Select defaults for the rest of the forms. Finally, review the complete cluster setup and click `Create` when satisfied. The cluster may take several minutes to become ready.

### Adding Nodes

Next we need to add worker nodes to the cluster. This is where the Onyx services will reside.

When the cluster is Active and while viewing the cluster, select the `Compute` tab and then `Add node group`.

![Adding a node group](/images/setup_guides/aws/eks/add_node_group.png)

First, provide a `Name` for the group (something like `onyx-node-group`). For `Node IAM role`, either select
an existing role that your organization has setup or create a new role. Whichever route we choose, we have to make sure
that the `AmazonEBSCSIDriverPolicy` is attached to the role, as this will be needed for our Persistent Volume Claims (PVCs)
to be fulfilled. If creating a role, all the other defaults provided by AWS should work (with the addition of the `AmazonEBSCSIDriverPolicy`). Give the role a name like `onyx-eks-nodegroup-role`.

![Configure group node](/images/setup_guides/aws/eks/configure_group_node.png)

Replace the `Instance Types` with `c5.2xlarge` machines (or `c5.4xlarge` if you are planning on scaling up beyond 100k documents). For disk, we recommend setting `Volume size` somewhere in the 200GB - 800GB range depending on how many documents you plan on indexing (storage is cheap). For most setups, we recommend setting the `Desired size` and `Minimum size` to 1, although you can increase this if needed to scale the cluster up once traffic picks up. Maximum unavailable can generally be left as default.

After reviewing, keep the default for the Networking section, and then proceed through to `Create`!

![Setting compute and scaling](/images/setup_guides/aws/eks/set_compute_and_scaling.png)

This may take up to 15 minutes for the compute nodes to come online.

### Create and Connect a User

You will need to create an IAM user that will have access AWS and the cluster from the command line.

Navigate to the `IAM Dashboard` found [here](https://console.aws.amazon.com/iam/). Select `Users` on the left sidebar and then `Create user`.

![Create user](/images/setup_guides/aws/eks/iam_dashboard.png)

Give the user a name like `onyx-eks-user`. For the user permissions, click `Attach policies directly` and provide the following permissions:

- `AmazonEKSClusterPolicy`
- `AmazonEKSServicePolicy`

Finishing reviewing and creating the user.

Back on the user's page, click into the newly created user, and then select `Create access key`.

![Create access key](/images/setup_guides/aws/eks/create_access_key.png)

Follow the process for creating the access key and secret. Select the `Command Line Interferace (CLI)` option during creation. Be sure to save the `Access key` and `Secret access key` for later.

Navigate back to the EKS cluster and select `Access` and then `Create access entry`.

![EKS access entry](/images/setup_guides/aws/eks/eks_access_entry.png)

In the `IAM principal` select the IAM ARN that you just created. Then click next.

![EKS IAM ARN](/images/setup_guides/aws/eks/eks_iam_arn.png)

For the Access policies, ensure that the `Policy name` is set to `AmazonEKSClusterAdminPolicy` and then click next. Review and then create the Access Policy.

![EKS policy](/images/setup_guides/aws/eks/eks_policy.png)

## Fetching the kubeconfig file

Log into the AWS CLI with `aws configure` and provide the access key and secret key from the IAM you created.

Then configure aws to connect with the cluster filling in the `region-code` and `cluster-name` in the following command

```shell
aws eks update-kubeconfig --region region-code --name cluster-name
```

Additional material found on the [AWS EKS documentation](https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html).

## Installing the Services

You will want to create a [namespace](https://kubernetes.io/blog/2016/08/kubernetes-namespaces-use-cases-insights/) for this to install. Namespaces are logical separations of grouped services for resource and permission managament.

```shell
kubectl create ns onyx
```

Next, we need to make sure that the gp2 storage class is set to the default storage class. This can be done by running the following command:

```shell
kubectl patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' -n onyx
```

Navigate to the `onyx/deployment/kubernetes` directory in the repository that you have cloned down

Afterwards, you can run the installation of the `yaml` files

```shell
kubectl apply -f . -n onyx
```

It may take a few minutes for the all the services to come online. To monitor the progress, run the following command:

```shell
kubectl -n onyx get pods
```

To check the status of the API server (usually the last to come online), run the following command:

```shell
kubectl -n onyx get pods | grep api-server | awk '{print $1}' | xargs -I {} kubectl -n onyx logs {} -f
```

You can navigate find the nginx load balancer by running the following command and then navigating to the url provided.

```shell
kubectl get svc -n onyx | grep nginx-service | awk '{print $4}'
```

# Move Somewhere Else Later

Below is an updated, step-by-step summary incorporating all necessary steps, including environment variables, SSL configuration, and the CA bundle for Aurora PostgreSQL IAM authentication:

---

### 1. Enable IAM Auth on the Aurora Cluster

In the RDS console, modify your Aurora cluster and enable **"IAM database authentication."**

### 2. Create and Configure a Database User

Connect with your master user (using the master password or IAM auth for the master user) and run:

```sql
CREATE ROLE mydbuser LOGIN;
GRANT rds_iam TO mydbuser;
ALTER ROLE mydbuser WITH NOPASSWORD;
```

This configures `mydbuser` to use IAM authentication rather than a static password.

### 3. Retrieve the Cluster Resource ID

Run:

```bash
aws rds describe-db-clusters --db-cluster-identifier <your-cluster-name> --region <your-region>
```

From the output, note the `"DbClusterResourceId"` value.

### 4. Create or Update an IAM Policy for `rds-db:connect`

Attach a policy to the IAM principal (user or role) that you’ll use for connecting:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "rds-db:connect",
      "Resource": "arn:aws:rds-db:<region>:<account_id>:dbuser:<DbClusterResourceId>/mydbuser"
    }
  ]
}
```

Replace `<region>`, `<account_id>`, `<DbClusterResourceId>`, and `mydbuser` with your actual values.

### 5. Obtain the RDS CA Certificate Bundle

Download the RDS CA certificate bundle for your region. For example, from the AWS docs you can get `rds-combined-ca-bundle.pem` or region-specific bundles such as `us-east-2-bundle.pem`.  
Save it locally, e.g.:

```
us-east-2-bundle.pem
```

This is required to verify the SSL connection to the database.

### 6. Generate a Short-Lived IAM Auth Token

Use the AWS CLI to generate the token:

```bash
TOKEN=$(aws rds generate-db-auth-token \
  --hostname <cluster-endpoint> \
  --port 5432 \
  --username mydbuser \
  --region <your-region>)
```

### 7. Set Environment Variables for Your Application

If using a code-based solution (e.g., SQLAlchemy), set environment variables to configure IAM auth and SSL:

```bash
export USE_IAM_AUTH=true
export AWS_REGION="<your-region>"    # matches your Aurora cluster's region
export POSTGRES_HOST="<cluster-endpoint>"
export POSTGRES_PORT="5432"
export POSTGRES_DB="<db-name>"
export POSTGRES_USER="mydbuser"

# If using IAM roles or AWS credentials:
export AWS_ACCESS_KEY_ID="<your-access-key-id>"
export AWS_SECRET_ACCESS_KEY="<your-secret-access-key>"
# If using STS temporary credentials:
# export AWS_SESSION_TOKEN="<your-session-token>"

# Provide the CA bundle for SSL verification.
# Your code should reference this file to create an SSL context.
# Ensure the file is accessible (correct path and permissions).
```

Your code should then load these environment variables, use `USE_IAM_AUTH=true` to trigger token-based authentication, and configure SSL using the provided CA bundle.

### 8. Connect Using psql with IAM Auth Manually (Optional Check)

If you'd like to test connectivity directly before running your code:

```bash
psql "host=<cluster-endpoint> port=5432 dbname=<db-name> user=mydbuser sslmode=require password=$TOKEN"
```

This should prompt no password errors and allow a secure, verified SSL connection to Aurora using your short-lived IAM token.

### 9. Ensure Proper Permissions and Migration Steps (If Needed)

- If running migrations (e.g., Alembic), ensure that `mydbuser` has the necessary permissions:
  ```sql
  GRANT CREATE ON DATABASE <db-name> TO mydbuser;
  GRANT USAGE ON SCHEMA public TO mydbuser;
  GRANT CREATE ON SCHEMA public TO mydbuser;
  ```
- Confirm you're using `sslmode=require` for synchronous connections with `psycopg2`. For asynchronous connections using `asyncpg`, provide an `ssl.SSLContext` created from the CA bundle.

---

If all steps are followed correctly—enabling IAM, configuring the database user and IAM policy, setting environment variables, and using the CA bundle for SSL—IAM authentication with Aurora PostgreSQL should succeed, allowing secure, passwordless (but token-based) authentication for your application.
